{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "71e0e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7ca95",
   "metadata": {},
   "source": [
    "### Импортируем датасет и предобученную модель русского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "68fa2135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.3 s, sys: 2.01 s, total: 55.3 s\n",
      "Wall time: 59.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataframe = pd.read_excel('/Users/magomednikolaev/Documents/Работа/NLP/we/comments.xlsx')\n",
    "russian_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/magomednikolaev/Documents/Работа/NLP/we/we/186/model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd8b12",
   "metadata": {},
   "source": [
    "### Обучим модель на уже имеющихся данных "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154d207",
   "metadata": {},
   "source": [
    "Удалим не имеющие отношения к делу атрибуты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "243cb9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframe['text_init']\n",
    "del dataframe['text_init_TOK']\n",
    "del dataframe['bast_list']\n",
    "del dataframe['word_cnt']\n",
    "\n",
    "dataframe = dataframe[['text_TOK_str', 'rating_cat', 'is_informative']]\n",
    "dataframe = dataframe.rename(columns = {'text_TOK_str' : 'text',\n",
    "                          'rating_cat' : 'tone',\n",
    "                          'is_informative' : 'informative'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203b0979",
   "metadata": {},
   "source": [
    "Пока возьмем только тональность "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c073182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeTone = dataframe.drop({'informative'}, axis = 1) \n",
    "dataframeInf = dataframe.drop({'tone'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ddce3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeTone_train, dataframeTone_test = train_test_split(dataframeTone, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81bf16",
   "metadata": {},
   "source": [
    "Будем использовать tf-idf vectorizer, ранее эта модель показала наилучшие параметры при ngram_range=(1, 4), поэтому пока остаивим этот параметр "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bd7eaf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer = TfidfVectorizer(\n",
    "                                   ngram_range=(1, 4), \n",
    "                                   lowercase=True, max_features=150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bf27ede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 60 ms, total: 1.12 s\n",
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = text_transformer.fit_transform(dataframeTone_train['text'])\n",
    "X_test = text_transformer.transform(dataframeTone_test['text'])\n",
    "y_train = dataframeTone_train['tone']\n",
    "y_test = dataframeTone_test['tone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8577fb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4176, 150000), (1045, 150000))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d555a",
   "metadata": {},
   "source": [
    "В качестве модели возьмем логистическую регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f139e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4)\n",
    "LogReg.fit(X_train, y_train)\n",
    "y_pred = LogReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "89d65b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.85      0.92      0.88       496\n",
      "     class 1       0.89      0.82      0.85       403\n",
      "     class 2       0.88      0.79      0.83       146\n",
      "\n",
      "    accuracy                           0.87      1045\n",
      "   macro avg       0.87      0.85      0.86      1045\n",
      "weighted avg       0.87      0.87      0.86      1045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b84cf",
   "metadata": {},
   "source": [
    "### Теперь попробуем аугментировать данные и посмотрим на качество модели после этого"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620c8ee",
   "metadata": {},
   "source": [
    "Напишем функцию(сопрем с гитхаба: https://github.com/akutuzov/webvectors/blob/master/preprocessing/rus_preprocessing_mystem.py), которая будет переводить mystem tags в upos tags, которые используются в нужных моделях. Несуществующие слова просто пропускаются, также, как и знаки препинания. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7ea8e6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the model...\n",
      "Processing input...\n"
     ]
    }
   ],
   "source": [
    "def tag_mystem(\n",
    "    text=\"Текст нужно передать функции в виде строки!\", mapping=None, postags=True\n",
    "):\n",
    "    # если частеречные тэги не нужны (например, их нет в модели), выставьте postags=False\n",
    "    # в этом случае на выход будут поданы только леммы\n",
    "\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(\",\")[0]\n",
    "            pos = pos.split(\"=\")[0].strip()\n",
    "            if mapping:\n",
    "                if pos in mapping:\n",
    "                    if pos == 'STOP':\n",
    "                        break\n",
    "                    pos = mapping[pos]  # здесь мы конвертируем тэги\n",
    "                else:\n",
    "                    pos = \"X\"  # на случай, если попадется тэг, которого нет в маппинге\n",
    "            tagged.append(lemma.lower() + \"_\" + pos)\n",
    "        except KeyError:\n",
    "            continue  # я здесь пропускаю знаки препинания, но вы можете поступить по-другому\n",
    "        except IndexError:\n",
    "            continue # я здесь пропускаю слова с опечатками и просто несуществующие\n",
    "            \n",
    "    if not postags:\n",
    "        tagged = [t.split(\"_\")[0] for t in tagged]\n",
    "    return tagged\n",
    "\n",
    "\n",
    "# Таблица преобразования частеречных тэгов Mystem в тэги UPoS:\n",
    "mapping_url = \"https://raw.githubusercontent.com/akutuzov/universal-pos-tags/4653e8a9154e93fe2f417c7fdb7a357b7d6ce333/ru-rnc.map\"\n",
    "\n",
    "mystem2upos = {}\n",
    "r = requests.get(mapping_url, stream=True)\n",
    "for pair in r.text.split(\"\\n\"):\n",
    "    pair = pair.split()\n",
    "    if len(pair) > 1:\n",
    "        mystem2upos[pair[0]] = pair[1]\n",
    "\n",
    "print(\"Loading the model...\", file=sys.stderr)\n",
    "m = Mystem()\n",
    "\n",
    "print(\"Processing input...\", file=sys.stderr)\n",
    "for line in sys.stdin:\n",
    "    res = line.strip()\n",
    "    output = tag_mystem(text=res, mapping=mystem2upos)\n",
    "    print(\" \".join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db08cec",
   "metadata": {},
   "source": [
    "Посмотрим на возможные тэги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b517b623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['ADJ', 'ADV', 'ADV', 'ADJ', 'DET', 'ADJ', 'SCONJ', 'INTJ', 'X', 'NUM', 'PART', 'ADP', 'NOUN', 'PRON', 'X', 'VERB'])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem2upos.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf4e1f",
   "metadata": {},
   "source": [
    "Присвоим тэг всему тексту, каждому слову"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1ec5b495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Пропускаем этот шаг, так как разбиение на выборки мы сдлеали заранее. \n",
    "\n",
    "# dataframeAug = dataframe.copy()\n",
    "# dataframeAug.text = dataframeAug.text.apply(lambda x: tag_mystem(x, mapping = mystem2upos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f08ecaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeTrainAug = dataframeTone_train.copy()\n",
    "dataframeTrainAug.text = dataframeTrainAug.text.apply(lambda x: tag_mystem(x, mapping = mystem2upos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64413c0",
   "metadata": {},
   "source": [
    "Посмотрим для начала, с какими проблемами может столкунться модель:\n",
    "   1. Уже на этапе тэга не все слова могут найтись\n",
    "   2. Даже по правильным тэгам не может найти \n",
    "   3. После применения замены появпялются \"-\"\n",
    "   4. Слова с опечатками не проходят на одном из этапов: на моменте тэга(но может и пройти), на моменте поиска синонимичного слова(там уже при наличии ошибкии тэга, скорее всего ничего не сработает)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e6e1b",
   "metadata": {},
   "source": [
    "Саугментируем тренировочный корпус, соеденим два тренировочных датасета, а скор оценим на тестовом датасете\n",
    "\n",
    "1. X_train is dataframeTone_train['text']\n",
    "2. X_test = is dataframeTone_test['text']\n",
    "3. y_train = dataframeTone_train['tone']\n",
    "4. y_test = dataframeTone_test['tone']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ed4b0e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 631/4176 [04:47<19:15,  3.07it/s]  /Users/magomednikolaev/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 4176/4176 [27:55<00:00,  2.49it/s]  \n"
     ]
    }
   ],
   "source": [
    "def replacement_by_WE(pandas_Series):\n",
    "    \n",
    "    new_corpus = list()\n",
    "\n",
    "    for text in tqdm(pandas_Series):\n",
    "\n",
    "        new_text = list()\n",
    "        for word in text:\n",
    "            try:\n",
    "                new_word = russian_model.most_similar(word)[0][0].split('_')[0]\n",
    "                new_word = re.sub('-', '', new_word)\n",
    "                new_word = re.sub(',', '', new_word)\n",
    "            except KeyError:\n",
    "                new_word = word.split('_')[0]\n",
    "\n",
    "            new_text.append(new_word)\n",
    "        new_corpus.append(' '.join(new_text))\n",
    "        \n",
    "    return new_corpus\n",
    "    \n",
    "    \n",
    "dataframeTrainAug['text'] = replacement_by_WE(dataframeTrainAug.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "68bb3f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Стало:',\n",
       " 'сделаюотреть регулировка роль быстро простой при огромный колличеств окошко жуткий тормоза',\n",
       " 'Было:',\n",
       " 'сделать настройка роль быстро просто при большой количество окно ужасный тормоз')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Стало:\", dataframeTrainAug.iloc[4000][0], \"Было:\", dataframeTone_train.iloc[4000][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ddc2b",
   "metadata": {},
   "source": [
    "Скачаю аугментированный датасет,чтобы заново не обучать модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bdb378ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeTrainAug.to_csv(\"/Users/magomednikolaev/Documents/Работа/NLP/we/comments_train_augmeted_all_tags.xlsx\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37adda",
   "metadata": {},
   "source": [
    "Чепуха:\n",
    "  1. ,д\n",
    "  2. ,з\n",
    "  3. .таки\n",
    "  4. ...ть\n",
    "  5. б;льший"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ce1ebdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пропускаем этот шаг, так как разбиение на выборки мы сдлеали заранее. \n",
    "\n",
    "# dataframe_concated = pd.concat([dataframeTrainAug, dataframeTone_train])\n",
    "# dataframe_concated = dataframe_concated.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0481140",
   "metadata": {},
   "source": [
    "### Переобучим модель на аугментированных данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "afc5f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пропускаем этот шаг, так как разбиение на выборки мы сдлеали заранее. \n",
    "\n",
    "# dataframe_concatedTone = dataframe_concated.drop({'informative'}, axis = 1) \n",
    "# dataframe_concatedInf = dataframe_concated.drop({'tone'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c9662827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пропускаем этот шаг, так как разбиение на выборки мы сдлеали заранее. \n",
    "\n",
    "# dataframe_concatedTone_train, dataframe_concatedTone_test = train_test_split(dataframe_concatedTone, \n",
    "#                                                                              test_size = 0.2, \n",
    "#                                                                              random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "28e45971",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_concatedTone_train = pd.concat([dataframeTrainAug, dataframeTone_train])\n",
    "dataframe_concatedTone_train = dataframe_concatedTone_train.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "89ec9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer = TfidfVectorizer(\n",
    "                                   ngram_range=(1, 4), \n",
    "                                   lowercase=True, \n",
    "                                   max_features=150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "26f12b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 s, sys: 103 ms, total: 2.18 s\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Xaug_train = text_transformer.fit_transform(dataframe_concatedTone_train['text'])\n",
    "Xaug_test = text_transformer.transform(dataframeTone_test['text']) # Как для прошлой модели\n",
    "yaug_train = dataframe_concatedTone_train['tone']\n",
    "yaug_test = dataframeTone_test['tone'] # Как для прошлой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ffde40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4)\n",
    "LogReg.fit(Xaug_train, yaug_train)\n",
    "yuag_pred = LogReg.predict(Xaug_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5cb4f462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.85      0.90      0.88       496\n",
      "     class 1       0.89      0.83      0.85       403\n",
      "     class 2       0.87      0.84      0.85       146\n",
      "\n",
      "    accuracy                           0.87      1045\n",
      "   macro avg       0.87      0.86      0.86      1045\n",
      "weighted avg       0.87      0.87      0.86      1045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(yaug_test, yuag_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc02c5",
   "metadata": {},
   "source": [
    "Сравним с предыдущим скором, как мы можем видеть качество модели по некоторым п\n",
    "унктам выросло, но очень не сильно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a0820530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.85      0.92      0.88       496\n",
      "     class 1       0.89      0.82      0.85       403\n",
      "     class 2       0.88      0.79      0.83       146\n",
      "\n",
      "    accuracy                           0.87      1045\n",
      "   macro avg       0.87      0.85      0.86      1045\n",
      "weighted avg       0.87      0.87      0.86      1045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28688c5f",
   "metadata": {},
   "source": [
    "Таким образом, модель стала работать лучше, хотя возможно, она переобучилась. Далее предстоит провести кросс валидацию, попробовать изменять гипер-параметры и стратегию аугментации(брать не топ 1 по близости(например)).\n",
    "Полезные ссылки:\n",
    "1. https://www.kaggle.com/theoviel/using-word-embeddings-for-data-augmentation\n",
    "2. http://www.machinelearning.ru/wiki/images/b/b3/Word2Vec.pdf\n",
    "3. https://github.com/akutuzov/webvectors\n",
    "4. https://universaldependencies.org/u/pos/\n",
    "\n",
    "СДЕЛАНО: Возможно, она будет работаь еще хуже, если агументировать отдельно и тестовую выборку и трейн выборки и уже сравнивать метрики по одним и тем же выборкам\n",
    "\n",
    "svc = SVC()\n",
    "lsvc = LinearSVC(random_state=123)\n",
    "rforest = RandomForestClassifier(random_state=123)\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "Возможно, высокий скор из-за того, что я не использовал ту же тестовую выборку, то я считаю. \n",
    "\n",
    "Поресечрчить зависимость скора от аугментации мусорных слов и наоборот с eli5.\n",
    "\n",
    "Поменять solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4fdbea",
   "metadata": {},
   "source": [
    "### Кросс-валидация "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4250e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6430a425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 130 ms, sys: 103 ms, total: 233 ms\n",
      "Wall time: 44.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv_results = cross_val_score(LogReg, Xaug_train, yaug_train, cv=skf, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726882f0",
   "metadata": {},
   "source": [
    "Приятно видеть, что кросс-валидация более или менее стабильна на всех фолдах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "16a75712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.89143546, 0.88661037, 0.8986731 , 0.88292094, 0.89438745]),\n",
       " 0.8908054654268582)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results, cv_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d430dc9",
   "metadata": {},
   "source": [
    "### Параметризация аугментации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1795fef",
   "metadata": {},
   "source": [
    "Теперь попробуем вычислить скор, если аугментировать не все подряд, а только интересующие тэги: наречия и прилагательные "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3a8e2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem2uposMod = {'A': ' ADJ',\n",
    " 'ADV': 'ADV',\n",
    " 'ADVPRO': 'STOP',\n",
    " 'ANUM': 'STOP',\n",
    " 'APRO': 'STOP',\n",
    " 'COM': 'STOP',\n",
    " 'CONJ': 'STOP',\n",
    " 'INTJ': 'STOP',\n",
    " 'NONLEX': 'STOP',\n",
    " 'NUM': 'STOP',\n",
    " 'PART': 'STOP',\n",
    " 'PR': 'STOP',\n",
    " 'S': 'STOP',\n",
    " 'SPRO': 'STOP',\n",
    " 'UNKN': 'X',\n",
    " 'V': 'STOP'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e8b6d",
   "metadata": {},
   "source": [
    "Скопируем все незакоментированное для аугментированной модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3b6237e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4176/4176 [02:20<00:00, 29.66it/s]\n"
     ]
    }
   ],
   "source": [
    "dataframeTrainAug = dataframeTone_train.copy()\n",
    "dataframeTrainAug.text = dataframeTrainAug.text.apply(lambda x: tag_mystem(x, mapping = mystem2uposMod))\n",
    "\n",
    "dataframeTrainAug['text'] = replacement_by_WE(dataframeTrainAug.text) \n",
    "dataframeTrainAug.to_csv(\"/Users/magomednikolaev/Documents/Работа/NLP/we/comments_train_augmeted_aadv_tags.xlsx\",) \n",
    "dataframe_concatedTone_train = pd.concat([dataframeTrainAug, dataframeTone_train])\n",
    "dataframe_concatedTone_train = dataframe_concatedTone_train.drop_duplicates(subset=['text'])\n",
    "\n",
    "Xaug_aadj_train = text_transformer.fit_transform(dataframe_concatedTone_train['text'])\n",
    "Xaug_aadj_test = text_transformer.transform(dataframeTone_test['text']) # Как для прошлой модели\n",
    "yaug_aadj_train = dataframe_concatedTone_train['tone']\n",
    "yaug_aadj_test = dataframeTone_test['tone'] # Как для прошлой модели\n",
    "\n",
    "LogReg = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4)\n",
    "LogReg.fit(Xaug_aadj_train, yaug_aadj_train)\n",
    "yuag_aadj_pred = LogReg.predict(Xaug_aadj_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6b0e0c",
   "metadata": {},
   "source": [
    "Без аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1e8dd56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.85      0.90      0.88       496\n",
      "     class 1       0.89      0.83      0.85       403\n",
      "     class 2       0.87      0.84      0.85       146\n",
      "\n",
      "    accuracy                           0.87      1045\n",
      "   macro avg       0.87      0.86      0.86      1045\n",
      "weighted avg       0.87      0.87      0.86      1045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(yaug_test, yuag_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc70b5",
   "metadata": {},
   "source": [
    "Аугментация по всем тэгам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f7de555e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.85      0.92      0.88       496\n",
      "     class 1       0.89      0.82      0.85       403\n",
      "     class 2       0.88      0.79      0.83       146\n",
      "\n",
      "    accuracy                           0.87      1045\n",
      "   macro avg       0.87      0.85      0.86      1045\n",
      "weighted avg       0.87      0.87      0.86      1045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c1288",
   "metadata": {},
   "source": [
    "Аугментация по прилагательным и наречиям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "bb0311a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.86      0.91      0.88       496\n",
      "     class 1       0.88      0.84      0.86       403\n",
      "     class 2       0.88      0.83      0.85       146\n",
      "\n",
      "    accuracy                           0.87      1045\n",
      "   macro avg       0.87      0.86      0.86      1045\n",
      "weighted avg       0.87      0.87      0.87      1045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magomednikolaev/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/magomednikolaev/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/magomednikolaev/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/magomednikolaev/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/magomednikolaev/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/magomednikolaev/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/magomednikolaev/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(yaug_aadj_test, yuag_aadj_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17353c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d157f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e557d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
