{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ed6a35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "# from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ded4a",
   "metadata": {},
   "source": [
    "### Импортируем датасет и предобученную модель русского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e672fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.2 s, sys: 1.66 s, total: 53.8 s\n",
      "Wall time: 57.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataframe = pd.read_excel('/Users/magomednikolaev/Documents/Работа/NLP/we/comments.xlsx')\n",
    "russian_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/magomednikolaev/Documents/Работа/NLP/we/we/186/model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d24783",
   "metadata": {},
   "source": [
    "### Обучим модель на уже имеющихся данных "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7596b3",
   "metadata": {},
   "source": [
    "Удалим не имеющие отношения к делу атрибуты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7d86e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframe['text_init']\n",
    "del dataframe['text_init_TOK']\n",
    "del dataframe['bast_list']\n",
    "del dataframe['word_cnt']\n",
    "\n",
    "dataframe = dataframe[['text_TOK_str', 'rating_cat', 'is_informative']]\n",
    "dataframe = dataframe.rename(columns = {'text_TOK_str' : 'text',\n",
    "                          'rating_cat' : 'tone',\n",
    "                          'is_informative' : 'informative'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad447626",
   "metadata": {},
   "source": [
    "Пока возьмем только тональность "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5d3b8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeTone = dataframe.drop({'informative'}, axis = 1) \n",
    "dataframeInf = dataframe.drop({'tone'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58f754e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeTone_train, dataframeTone_test = train_test_split(dataframeTone, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4567db90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4176, 1045)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0865d5",
   "metadata": {},
   "source": [
    "Будем использовать tf-idf vectorizer, ранее эта модель показала наилучшие параметры при ngram_range=(1, 4), поэтому пока остаивим этот параметр "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fdd85698",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer = TfidfVectorizer(\n",
    "                                   ngram_range=(1, 4), \n",
    "                                   lowercase=True, max_features=150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3050ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 s, sys: 69.3 ms, total: 1.11 s\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = text_transformer.fit_transform(dataframeTone_train['text'])\n",
    "X_test = text_transformer.transform(dataframeTone_test['text'])\n",
    "y_train = dataframeTone_train['tone']\n",
    "y_test = dataframeTone_test['tone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3add29fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4176, 150000), (1045, 150000))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a62a0b",
   "metadata": {},
   "source": [
    "В качестве модели возьмем логистическую регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c151e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4)\n",
    "LogReg.fit(X_train, y_train)\n",
    "y_pred = LogReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d30f1fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.85      0.92      0.88       496\n",
      "     class 1       0.89      0.82      0.85       403\n",
      "     class 2       0.88      0.79      0.83       146\n",
      "\n",
      "    accuracy                           0.87      1045\n",
      "   macro avg       0.87      0.85      0.86      1045\n",
      "weighted avg       0.87      0.87      0.86      1045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magomednikolaev/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6a6fe",
   "metadata": {},
   "source": [
    "### Теперь попробуем аугментировать данные и посмотрим на качество модели после этого"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74336dcd",
   "metadata": {},
   "source": [
    "Напишем функцию(сопрем с гитхаба: https://github.com/akutuzov/webvectors/blob/master/preprocessing/rus_preprocessing_mystem.py), которая будет переводить mystem tags в upos tags, которые используются в нужных моделях. Несуществующие слова просто пропускаются, также, как и знаки препинания. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "65b92b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the model...\n",
      "Processing input...\n"
     ]
    }
   ],
   "source": [
    "def tag_mystem(\n",
    "    text=\"Текст нужно передать функции в виде строки!\", mapping=None, postags=True\n",
    "):\n",
    "    # если частеречные тэги не нужны (например, их нет в модели), выставьте postags=False\n",
    "    # в этом случае на выход будут поданы только леммы\n",
    "\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(\",\")[0]\n",
    "            pos = pos.split(\"=\")[0].strip()\n",
    "            if mapping:\n",
    "                if pos in mapping:\n",
    "                    pos = mapping[pos]  # здесь мы конвертируем тэги\n",
    "                else:\n",
    "                    pos = \"X\"  # на случай, если попадется тэг, которого нет в маппинге\n",
    "            tagged.append(lemma.lower() + \"_\" + pos)\n",
    "        except KeyError:\n",
    "            continue  # я здесь пропускаю знаки препинания, но вы можете поступить по-другому\n",
    "        except IndexError:\n",
    "            continue # я здесь пропускаю слова с опечатками и просто несуществующие\n",
    "            \n",
    "    if not postags:\n",
    "        tagged = [t.split(\"_\")[0] for t in tagged]\n",
    "    return tagged\n",
    "\n",
    "\n",
    "# Таблица преобразования частеречных тэгов Mystem в тэги UPoS:\n",
    "mapping_url = \"https://raw.githubusercontent.com/akutuzov/universal-pos-tags/4653e8a9154e93fe2f417c7fdb7a357b7d6ce333/ru-rnc.map\"\n",
    "\n",
    "mystem2upos = {}\n",
    "r = requests.get(mapping_url, stream=True)\n",
    "for pair in r.text.split(\"\\n\"):\n",
    "    pair = pair.split()\n",
    "    if len(pair) > 1:\n",
    "        mystem2upos[pair[0]] = pair[1]\n",
    "\n",
    "print(\"Loading the model...\", file=sys.stderr)\n",
    "m = Mystem()\n",
    "\n",
    "print(\"Processing input...\", file=sys.stderr)\n",
    "for line in sys.stdin:\n",
    "    res = line.strip()\n",
    "    output = tag_mystem(text=res, mapping=mystem2upos)\n",
    "    print(\" \".join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd106fc9",
   "metadata": {},
   "source": [
    "Посмотрим на возможные тэги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0fb79638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['ADJ', 'ADV', 'ADV', 'ADJ', 'DET', 'ADJ', 'SCONJ', 'INTJ', 'X', 'NUM', 'PART', 'ADP', 'NOUN', 'PRON', 'X', 'VERB'])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem2upos.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b88729",
   "metadata": {},
   "source": [
    "Присвоим тэг всему тексту, каждому слову"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "262e5b74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframeAug = dataframe.copy()\n",
    "dataframeAug.text = dataframeAug.text.apply(lambda x: tag_mystem(x, mapping = mystem2upos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2526832",
   "metadata": {},
   "source": [
    "Посмотрим для начала, с какими проблемами может столкунться модель:\n",
    "   1. Уже на этапе тэга не все слова могут найтись\n",
    "   2. Даже по правильным тэгам не может найти \n",
    "   3. После применения замены появпялются \"-\"\n",
    "   4. Слова с опечатками не проходят на одном из этапов: на моменте тэга(но может и пройти), на моменте поиска синонимичного слова(там уже при наличии ошибкии тэга, скорее всего ничего не сработает)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0067e52",
   "metadata": {},
   "source": [
    "Саугментируем данный корпус и соеденим два датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b24069af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframeAug.loc[1000, 'text'][0]    \n",
    "def replacement_by_WE(text):\n",
    "    \n",
    "    new_text = list()\n",
    "    for word in text:\n",
    "        try:\n",
    "            new_word = russian_model.most_similar(word)[0][0].split('_')[0]\n",
    "            new_word = re.sub('-', '', new_word)\n",
    "        except KeyError:\n",
    "            new_word = word.split(\"_\")[0]\n",
    "        new_text.append(new_word)\n",
    "    return new_text  \n",
    "\n",
    "# dataframeAug.text.apply(lambda x: replacement_by_WE(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f081e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5221/5221 [36:08<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "new_corpus = list()\n",
    "for text in tqdm(dataframeAug.text):\n",
    "    \n",
    "    new_text = list()\n",
    "    for word in text:\n",
    "        try:\n",
    "            new_word = russian_model.most_similar(word)[0][0].split('_')[0]\n",
    "            new_word = re.sub('-', '', new_word)\n",
    "        except KeyError:\n",
    "            new_word = word.split(\"_\")[0]\n",
    "            \n",
    "        new_text.append(new_word)\n",
    "    new_corpus.append(' '.join(new_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6fa1b2",
   "metadata": {},
   "source": [
    "Чепуха:\n",
    "  1. ,д\n",
    "  2. ,з\n",
    "  3. .таки\n",
    "  4. ...ть\n",
    "  5. б;льший"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "cb6cc0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeAug['text'] = new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f8fcced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_concated = pd.concat([dataframeAug, dataframe])\n",
    "dataframe_concated = dataframe_concated.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e33f3a",
   "metadata": {},
   "source": [
    "### Переобучим модель на аугментированных данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "cf50d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_concatedTone = dataframe_concated.drop({'informative'}, axis = 1) \n",
    "dataframe_concatedInf = dataframe_concated.drop({'tone'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "bda3fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_concatedTone_train, dataframe_concatedTone_test = train_test_split(dataframe_concatedTone, \n",
    "                                                                             test_size = 0.2, \n",
    "                                                                             random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "62540492",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer = TfidfVectorizer(\n",
    "                                   ngram_range=(1, 4), \n",
    "                                   lowercase=True, \n",
    "                                    max_features=150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "769ea9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.23 s, sys: 218 ms, total: 2.45 s\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Xaug_train = text_transformer.fit_transform(dataframe_concatedTone_train['text'])\n",
    "Xaug_test = text_transformer.transform(dataframe_concatedTone_test['text'])\n",
    "yaug_train = dataframe_concatedTone_train['tone']\n",
    "yaug_test = dataframe_concatedTone_test['tone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "89c44342",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4)\n",
    "LogReg.fit(Xaug_train, yaug_train)\n",
    "yuag_pred = LogReg.predict(Xaug_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "6ce84b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.87      0.94      0.90       956\n",
      "     class 1       0.92      0.87      0.89       884\n",
      "     class 2       0.94      0.85      0.89       230\n",
      "\n",
      "    accuracy                           0.90      2070\n",
      "   macro avg       0.91      0.88      0.90      2070\n",
      "weighted avg       0.90      0.90      0.90      2070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(yaug_test, yuag_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f17d0",
   "metadata": {},
   "source": [
    "Таким образом, модель стала работать лучше, хотя возможно, она переобучилась. Далее прдестоит провести кросс валидацию, попробовать изменять гипер-параметры и стратегию аугментации.\n",
    "Полезные ссылки:\n",
    "1. https://www.kaggle.com/theoviel/using-word-embeddings-for-data-augmentation\n",
    "2. http://www.machinelearning.ru/wiki/images/b/b3/Word2Vec.pdf\n",
    "3. https://github.com/akutuzov/webvectors\n",
    "4. https://universaldependencies.org/u/pos/\n",
    "\n",
    "Возможно, она будет работаь еще хуже, если агументировать отдельно и тестовую выборку и трейн выборки и уже сранвитьвать метрики по одним и тем же выборкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e1bb32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
